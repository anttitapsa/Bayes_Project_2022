---
title: "BDA project report"
author: "Amanda Aarnio, Anni Niskanen, Antti Huttunen"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    number_sections: true
    toc: yes
    toc_depth: 1
---

\newpage

# Introduction

Can the grade point averages (GPAs) of first-year college students be predicted based on high school GPAs, SAT scores and other factors? This report presents a solution to this problem by applying two statistical models to predict the first-year college GPA. These models are fitted in Stan and compared according to Bayesian data analysis concepts. 

The report begins with a description of the data and the problem. After this, the models are introduced and their performances are analysed. Model improvements are then discussed and conclusions are made. Finally, self-reflection of the project is presented.

# Data and Problem 

The data contains information from a sample of 219 first year students at a Midwestern college in 1996. The data has 10 variables:

```
GPA           First-year college GPA on a 0.0 to 4.0 scale
HSGPA         High school GPA on a 0.0 to 4.0 scale
SATV          Verbal/critical reading SAT score
SATM          Math SAT score
Male          1= male, 0= female
HU            Number of credit hours earned in humanities courses in high school
SS            Number of credit hours earned in social science courses in high 
              school
FirstGen      1= student is the first in her or his family to attend college,
              0=otherwise
White         1= white students, 0= others
CollegeBound  1=attended a high school where >=50% students intended to go on to
              college, 0=otherwise
```
The description of the data can be obtained from: https://vincentarelbundock.github.io/Rdatasets/doc/Stat2Data/FirstYearGPA.html. The data can be accessed in R from the Stat2Data package as follows:

```{r}
library(Stat2Data)
data("FirstYearGPA")
```

This report attempts to solve the problem of predicting the GPA of first-year college students using this data. Five variables were chosen to predict the first-year college GPA (GPA): HSGPA, SATV, SATM, HU and SS. All of the chosen variables have numerical values. In addition, the hierarchical model uses categorical variables Male and White to group the data into four different groups and predicts the GPA with group-level predictors. The barplot below represents these four groups and their sizes.

```{r libraries, warning=FALSE, message=FALSE}
# Importing packages
library(cmdstanr)
library(bayesplot)
library(ggplot2)
library(gridExtra)
library(loo)
```

```{r databarplot, fig.cap="The division of the data in four different groups. The amounts of the data points in the groups can be seen in bars.", echo=FALSE, fig.width=6, fig.height=4}
White <- FirstYearGPA$White
White <- gsub(1, "White", White)
White <- gsub(0, "Non-White", White)
Male <- FirstYearGPA$Male
Male <- gsub(1, "Male", Male)
Male <- gsub(0, "Female", Male)
counts <- data.frame(White=White, Male=Male)
ggplot(data = counts, aes(x = Male, fill=White)) + geom_bar() + 
  xlab("Gender") + scale_fill_brewer(palette = "Paired", name = "") 
```


# Models

In this project, two different models are utilised: a pooled and a hierarchical model. Both are are covered in following sections. The mathematical notations, Stan implementations, and Stan model runs are included in the sections. Both models are linear Gaussian models whose expected values $\mu_i$ are constructed as linear combinations of the chosen variables HSGPA, SATV, SATM, HU, and SS. The coefficients of these linear combinations are our parameters $\alpha$, $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$.

Weakly informative normal priors were used in both models. N(0,10) was the prior distribution for $\sigma$ in both models. In the pooled model, N(0,100) was the prior for both $\alpha$ and all $\beta_k$. The hierarchical model had similar priors: N(0,100) for all means $\mu_{\alpha}$ and $\mu_{\beta_k}$, and N(0,10) for all standard deviations $\sigma_{\alpha}$ and $\sigma_{\beta_k}$. We thought these priors reasonable for multiple reasons. Firstly, they all center on 0, which was thought wisest as we have no information whether the intercept $\alpha$ or the slopes $\beta_k$ should be positive or negative. Secondly, the standard deviations, 100 for the means and 10 for the standard deviations, were considered large enough to produce wide enough (but not too wide) prior distributions.

As can be seen from the code where the Stan models are run, default values were used for running the MCMC chains. That is, 4 chains of 2000 iterations were run, and the first 1000 iterations of each chain were considered warm-up.

## Pooled model

In the pooled model, the expected values $\mu_i$ are constructed using the common parameters $\alpha$ and $\beta_k$, which have the aforementioned weakly informative priors. The GPAs have a common standard deviation $\sigma$. 

**Mathematical notation**

$$
\begin{aligned}
GPA_i &\sim N(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot HSGPA_i + \beta_2 \cdot SATV_i + \beta_3 \cdot SATM_i + \beta_4 \cdot HU_i + \beta_5 \cdot SS_i \\
\sigma &\sim N(0, 10) \\
\alpha &\sim N(0, 100) \\
\beta_k &\sim N(0, 100) \\
\end{aligned}
$$

**Stan code**

```{stan, output.var="pool", eval = FALSE}
data {
  int<lower=0> N;
  matrix[N,5] x;
  vector[N] y;
  real musigma;
  real sigmasigma;
}

parameters {
  real alpha;
  vector[5] betas;
  real<lower=0> sigma;
}

transformed parameters {
  vector[N] mu;
  mu = alpha + betas[1]*x[,1] + betas[2]*x[,2] + betas[3]*x[,3] +
       betas[4]*x[,4] + betas[5]*x[,5];
}

model {
  // priors
  alpha ~ normal(0, musigma);
  betas ~ normal(0, musigma);
  sigma ~ normal(0, sigmasigma);
  
  // likelihood
  y ~ normal(mu, sigma);
}

generated quantities {
  vector[N] ypred;
  vector[N] log_lik;
  
  // Generate predictive distributions for GPA
  for (i in 1:N)
    ypred[i] = normal_rng(mu[i], sigma);
  
  // log likelihoods
  for (i in 1:N)
    log_lik[i] = normal_lpdf(y[i] | mu[i], sigma);
}
```

**Running the model**

```{r data_pooled, warning=FALSE, message=FALSE}
data_pooled <- list(N = nrow(FirstYearGPA),
                    x = subset(FirstYearGPA, select = c('HSGPA', 'SATV', 'SATM',
                                                        'HU','SS')),
                    y = FirstYearGPA$GPA,
                    musigma = 100,
                    sigmasigma = 10)
```

```{r pooled_model_fit}
mod_pooled <- cmdstan_model("pooled.stan")
fit_pooled <- mod_pooled$sample(data_pooled, refresh = 0, seed = 03091900)
```

## Hierarchical model

In the hierarchical model, students have been divided in four different groups: 1. white males, 2. non-white males, 3. white females, and 4. non-white females. The groups now have their own parameters $\alpha_j$ and $\beta_{kj}$, where $j$ denotes the group a student belongs to. The parameters $\alpha_j$ and $\beta_{kj}$ have common hyperparameters , implying the parameters can be different across groups, but they have something common as well.

**Mathematical notation**

$$
\begin{aligned}
GPA_{ij} &\sim N(\mu_{ij}, \sigma) \\
\mu_{ij} &= \alpha_j + \beta_{1j} \cdot HSGPA_i + \beta_{2j} \cdot SATV_i + \beta_{3j} \cdot SATM_i + \beta_{4j} \cdot HU_i + \beta_{5j} \cdot SS_i \\
\sigma &\sim N(0, 10) \\
\alpha_j &\sim N(\mu_{\alpha}, \sigma_{\alpha}) \\
\beta_{1j} &\sim N(\mu_{\beta_1}, \sigma_{\beta_1}) \\
\beta_{2j} &\sim N(\mu_{\beta_2}, \sigma_{\beta_2}) \\
\beta_{3j} &\sim N(\mu_{\beta_3}, \sigma_{\beta_3}) \\
\beta_{4j} &\sim N(\mu_{\beta_4}, \sigma_{\beta_4}) \\
\beta_{5j} &\sim N(\mu_{\beta_5}, \sigma_{\beta_5}) \\
\mu_{\alpha} &\sim N(0,100) \\
\sigma_{\alpha} &\sim N(0,10) \\
\mu_{\beta_k} &\sim N(0,100) \\
\sigma_{\beta_k} &\sim N(0,10) \\
\end{aligned}
$$

**Stan code**

```{stan, output.var="hier", eval = FALSE}
data {
  int<lower=0> N1;
  int<lower=0> N2;
  int<lower=0> N3;
  int<lower=0> N4;
  matrix[N1,5] x1;
  matrix[N2,5] x2;
  matrix[N3,5] x3;
  matrix[N4,5] x4;
  vector[N1] y1;
  vector[N2] y2;
  vector[N3] y3;
  vector[N4] y4;
  real musigma;
  real sigmasigma;
}

parameters {
  // parameters
  real alpha1;
  real alpha2;
  real alpha3;
  real alpha4;
  vector[5] betas1;
  vector[5] betas2;
  vector[5] betas3;
  vector[5] betas4;
  real<lower=0> sigma;
  
  // hyperparameters
  real pmualpha;
  real<lower=0> psalpha;
  vector[5] pmubetas;
  vector<lower=0>[5] psbetas;
}

transformed parameters {
  vector[N1] mu1 = alpha1 + betas1[1]*x1[,1] + betas1[2]*x1[,2] 
                   + betas1[3]*x1[,3] + betas1[4]*x1[,4] + betas1[5]*x1[,5];
  vector[N2] mu2 = alpha2 + betas2[1]*x2[,1] + betas2[2]*x2[,2] 
                   + betas2[3]*x2[,3] + betas2[4]*x2[,4] + betas2[5]*x2[,5];
  vector[N3] mu3 = alpha3 + betas3[1]*x3[,1] + betas3[2]*x3[,2] 
                   + betas3[3]*x3[,3] + betas3[4]*x3[,4] + betas3[5]*x3[,5];
  vector[N4] mu4 = alpha4 + betas4[1]*x4[,1] + betas4[2]*x4[,2] 
                   + betas4[3]*x4[,3] + betas4[4]*x4[,4] + betas4[5]*x4[,5];
}

model {
  // hyperpriors
  pmualpha ~ normal(0, musigma);
  psalpha ~ normal(0, sigmasigma);
  for (i in 1:5){
    pmubetas[i] ~ normal(0, musigma);
    psbetas[i] ~ normal(0, sigmasigma); 
  }
  
  // priors
  alpha1 ~ normal(pmualpha, psalpha);
  alpha2 ~ normal(pmualpha, psalpha);
  alpha3 ~ normal(pmualpha, psalpha);
  alpha4 ~ normal(pmualpha, psalpha);
  betas1 ~ normal(pmubetas, psbetas);
  betas2 ~ normal(pmubetas, psbetas);
  betas3 ~ normal(pmubetas, psbetas);
  betas4 ~ normal(pmubetas, psbetas);
  sigma ~ normal(0, sigmasigma);
  
  // likelihoods
  y1 ~ normal(mu1, sigma);
  y2 ~ normal(mu2, sigma);
  y3 ~ normal(mu3, sigma);
  y4 ~ normal(mu4, sigma);
}

generated quantities{
  vector[N1] ypred1;
  vector[N2] ypred2;
  vector[N3] ypred3;
  vector[N4] ypred4;
  vector[N1+N2+N3+N4] log_lik;
  
  // Generate predictive distributions for GPA
  for (i in 1:N1)
    ypred1[i] = normal_rng(mu1[i], sigma);
  for (i in 1:N2)
    ypred2[i] = normal_rng(mu2[i], sigma);
  for (i in 1:N3)
    ypred3[i] = normal_rng(mu3[i], sigma);
  for (i in 1:N4)
    ypred4[i] = normal_rng(mu4[i], sigma);
    
  // log likelihoods
  for (i in 1:N1)
    log_lik[i] = normal_lpdf(y1[i] | mu1[i], sigma);
  for (i in 1:N2)
    log_lik[N1+i] = normal_lpdf(y2[i] | mu2[i], sigma);
  for (i in 1:N3)
    log_lik[N1+N2+i] = normal_lpdf(y3[i] | mu3[i], sigma);
  for (i in 1:N4)
    log_lik[N1+N2+N3+i] = normal_lpdf(y4[i] | mu4[i], sigma);
}
```

**Running the model**

```{r data_hierarchical, warning=FALSE, message=FALSE}
male_white <- FirstYearGPA[FirstYearGPA$Male==1 & FirstYearGPA$White==1,]
male_non_white <- FirstYearGPA[FirstYearGPA$Male==1 & FirstYearGPA$White==0,]
female_white <- FirstYearGPA[FirstYearGPA$Male==0 & FirstYearGPA$White==1,]
female_non_white <- FirstYearGPA[FirstYearGPA$Male==0 & FirstYearGPA$White==0,]

data_hierarchical <- list(N1 = nrow(male_white),
                            N2 = nrow(male_non_white),
                            N3 = nrow(female_white),
                            N4 = nrow(female_non_white),
                            x1 = subset(male_white, select = c('HSGPA','SATV',
                                                               'SATM','HU',
                                                               'SS')),
                            x2 = subset(male_non_white, select = c('HSGPA','SATV', 
                                                                   'SATM','HU',
                                                                   'SS')),
                            x3 = subset(female_white, select = c('HSGPA','SATV',
                                                                 'SATM','HU',
                                                                 'SS')),
                            x4 = subset(female_non_white, select = c('HSGPA','SATV', 
                                                                     'SATM','HU',
                                                                     'SS')),
                            y1 = male_white$GPA,
                            y2 = male_non_white$GPA,
                            y3 = female_white$GPA,
                            y4 = female_non_white$GPA,
                            musigma = 100,
                            sigmasigma = 10)
```

```{r hierarchical_model_fit}
mod_hierarchical <- cmdstan_model("hierarchical.stan")
fit_hierarchical <-mod_hierarchical$sample(data_hierarchical, refresh = 0,
                                           seed = 03091900)
```


# Analysis and Results

This section contains the analysis of both models. The analysis includes convergence diagnostics, posterior predictive checks, and prior sensitivity analysis. This section also contains discussion about posterior distributions of the parameters and model comparison.

## Converge diagnostics

### Pooled model

```{r pooled_rhat, results='hold'}
summary_pooled <- fit_pooled$summary()
sum(summary_pooled$rhat > 1.01)
sum(summary_pooled$rhat < 0.99)
```

```{r pooled_ess, results='hold'}
mean(summary_pooled$ess_tail)
sum(summary_pooled$ess_tail<400)
mean(summary_pooled$ess_bulk)
sum(summary_pooled$ess_bulk<400)
```

None of the $\hat{R}$-values for the pooled model exceed 1.01, which indicates that there are no problems with the convergence of the simulated chains. The effective sample size (ESS) is over 400 (recommended threshold with four parallel chains, 100 per chain) for all of the ess_bulk and ess_tail values, which indicates that the $\hat{R}$-estimate can be trusted to make decisions about convergence and the quality of the chains. Finally, the model does not observe divergence.

### Hierarchical model

```{r hierarchical_rhat, results='hold'}
summary_hierarchical <- fit_hierarchical$summary()
sum(summary_hierarchical$rhat > 1.01)
sum(summary_hierarchical$rhat < 0.99)
```

```{r hierarchical_ess, results='hold'}
mean(summary_hierarchical$ess_tail)
sum(summary_hierarchical$ess_tail<400)
mean(summary_hierarchical$ess_bulk)
sum(summary_hierarchical$ess_bulk<400)
```

Similarly to the pooled model, in the hierarchical model, no $\hat{R}$-values exceed 1.01, indicating that the chains converge. A few ESS values are below 400 and the model gives a divergence of approximately 5%. These facts indicate that there might be some problems with the convergence of the model. However, the obtained values do not differ significantly from the desired values, so the probability of having problems with convergence is small.


## Posterior predictive checks

48 histograms of replications of the original data, i.e. the posterior predictive distributions, are shown for all models. In addition, the empirical probability density (epdf) and empirical cumulative distribution (ecdf) functions of the original data and 100 replications are overlapped and compared in order to investigate how well the posterior corresponds to the original data. The closer the replications are to the original data, the better predictions the fitted model can make.

### Pooled model

As can be seen in Figure \@ref(fig:pooledposterior), the replicated histograms have a similar distribution and width compared to the original data. Moreover, the replicated epdf and ecdf functions follow the shape of the epdf and ecdf of the original data, and variation around the original functions is quite low. Therefore the model posterior corresponds to the original data well and the model is good in posterior prediction.

```{r pooledposterior, fig.height=5, message=FALSE, warning=FALSE, fig.cap = "On the top of the figure 48 replications of the pooled model are shown with histograms. The original data is shown on the top left corner. Below the histograms, 100 epdf and ecdf distributions of the replications are overlapped with the epdf and ecdf of original data."}
y <- FirstYearGPA$GPA
ypred <- fit_pooled$draws("ypred", format = "matrix")

grid.arrange( ppc_hist(y, ypred[1:48,]),
              ppc_dens_overlay(y, ypred[1:100,]), 
              ppc_ecdf_overlay(y, ypred[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

### Hierarchical model

As can be seen when comparing Figures \@ref(fig:hierarchicalposterior1), \@ref(fig:hierarchicalposterior2), \@ref(fig:hierarchicalposterior3), and \@ref(fig:hierarchicalposterior4), the replications follow the original data better for white students than non-white students. The biggest reason for this is the smaller amount of data points in non-white groups. It can bee seen from the figures that replications of non-white groups have more width variation and more outliers than in the original data. In addition, ecdfs have more variation in non-white groups than in white groups.

Overall, the replications of the white groups are quite good, although not as good as in the pooled model, and the replications in non-white groups are not as good since they don't follow the original data as well.

```{r hierarchicalposterior1, fig.height=5, message=FALSE, warning=FALSE, fig.cap = "On the top of the figure 48 replications of the hierarchical model for the group of white male students are shown with histograms. The original data is shown on the top left corner. Below the histograms, 100 epdf and ecdf distributions of the replications are overlapped with the epdf and ecdf of original data."}
y1 <- data_hierarchical$y1
ypred1 <- fit_hierarchical$draws("ypred1", format = "matrix")

grid.arrange(ppc_hist(y1, ypred1[1:48,]),
ppc_dens_overlay(y1, ypred1[1:100,]),
ppc_ecdf_overlay(y1, ypred1[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

```{r hierarchicalposterior2, fig.height=5, message=FALSE, warning=FALSE, fig.cap = "On the top of the figure 48 replications of the hierarchical model for the group of non-white male students are shown with histograms. The original data is shown on the top left corner. Below the histograms, 100 epdf and ecdf distributions of the replications are overlapped with the epdf and ecdf of original data."}
y2 <- data_hierarchical$y2
ypred2 <- fit_hierarchical$draws("ypred2", format = "matrix")

grid.arrange(ppc_hist(y2, ypred2[1:48,]),
ppc_dens_overlay(y2, ypred2[1:100,]),
ppc_ecdf_overlay(y2, ypred2[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

```{r hierarchicalposterior3, fig.height=5, message=FALSE, warning=FALSE, fig.cap = "On the top of the figure 48 replications of the hierarchical model for the group of white female students are shown with histograms. The original data is shown on the top left corner. Below the histograms, 100 epdf and ecdf distributions of the replications are overlapped with the epdf and ecdf of original data."}
y3 <- data_hierarchical$y3
ypred3 <- fit_hierarchical$draws("ypred3", format = "matrix")

grid.arrange(ppc_hist(y3, ypred3[1:48,]),
ppc_dens_overlay(y3, ypred3[1:100,]),
ppc_ecdf_overlay(y3, ypred3[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

```{r hierarchicalposterior4, fig.height=5, message=FALSE, warning=FALSE, fig.cap = "On the top of the figure 48 replications of the hierarchical model for the group of non-white female students are shown with histograms. The original data is shown on the top left corner. Below the histograms, 100 epdf and ecdf distributions of the replications are overlapped with the epdf and ecdf of original data."}
y4 <- data_hierarchical$y4
ypred4 <- fit_hierarchical$draws("ypred4", format = "matrix")

grid.arrange(ppc_hist(y4, ypred4[1:48,]),
ppc_dens_overlay(y4, ypred4[1:100,]),
ppc_ecdf_overlay(y4, ypred4[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

\newpage

## Predictive performance assessment

Predictive performance assessment was not performed for the models. The reason for this is that the models are regression models, and no sensible metrics for examining the performance of regression models exist. Theoretically, a metric such as mean absolute error (MAE) or mean squared error (MSE) between the real and predicted values could be calculated for the data. This is not common practice, however, so predictive performance assessment is simply skipped here.


## Prior sensitivity analysis

The original priors utilised were N$(0,100)$ for $\alpha$ and $\beta$ or their means, and N$(0,10)$ for all standard deviations. For prior sensitivity analysis, the priors presented in the table below were tested to see how much the posteriors of $\alpha$ and $\beta_k$ would differ from the original. Altogether, 8 prior combinations were tested, including the original priors for reference. The same prior combinations were used for testing both models. Narrower priors, e.g. N$(0,0.1)$, were excluded from the analysis, as they would not be weakly informative anymore.

```{r posterior_sensitivity1, echo=FALSE, message=FALSE, warning=FALSE}
library(kableExtra)
musigmas <- c(100, 1000, 50, 10, 1, 100, 100, 100)
sigmasigmas <- c(10, 10, 10, 10, 10, 100, 50, 1)

dimnames <- list(c("alpha, beta OR mu_alpha, mu_beta", "sigma, sigma_alpha, sigma_beta"),
                 c("1", "2", "3", "4", "5", "6", "7", "8"))
prior_table <- matrix(nrow = 2, ncol = length(musigmas), dimnames = dimnames)

for (i in 1:length(musigmas)) {
  prior_table[1, i] <- paste0("N(0,", musigmas[i], ")")
  prior_table[2, i] <- paste0("N(0,", sigmasigmas[i], ")")
}

prior_table <- as.data.frame(prior_table)
kable(t(prior_table), format = "latex", caption = "Used priors for sensitivity analysis")
```

### Pooled model

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Takes ages to run, beware!
dimnames <- list(c("alpha", "beta1", "beta2", "beta3", "beta4", "beta5"),
                 c("1", "2", "3", "4", "5", "6", "7", "8"))
mus_pooled <- matrix(nrow = 6, ncol = length(musigmas), dimnames = dimnames)
sigmas_pooled <- matrix(nrow = 6, ncol = length(musigmas), dimnames = dimnames)

for (i in 1:length(musigmas)) { 
  data_pooled_sens <- list(N = nrow(FirstYearGPA),
                         x = subset(FirstYearGPA, select = c('HSGPA', 'SATV',
                                                             'SATM','HU','SS')),
                         y = FirstYearGPA$GPA,
                         musigma = musigmas[i],
                         sigmasigma = sigmasigmas[i])
  fit_pooled_sens <- mod_pooled$sample(data_pooled_sens, refresh = 0,
                                       seed = 03091900)
  summary_sens <- fit_pooled_sens$summary()
  mus_pooled[,i] <- summary_sens$mean[2:7]
  sigmas_pooled[,i] <- summary_sens$sd[2:7]
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(as.data.frame(mus_pooled), format = "latex", caption = "Sensitivity analysis of the pooled model: Posterior means")
kable(as.data.frame(sigmas_pooled), format = "latex", caption = "Sensitivity analysis of the pooled model: Posterior SDs")
```

The pooled model was run with the 8 prior combinations. The resulting posterior means and SDs for all parameters are reported in the two tables above. The results show that regardless of the used priors, all posteriors for $\alpha$ and $\beta_k$ are very similar to each other. Therefore it can be concluded that the pooled model is not sensitive to changes in the priors.

### Hierarchical model

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Takes ages to run, beware!
dimnames <- list(c("alpha1", "alpha2", "alpha3", "alpha4",
                   "beta1_1", "beta2_1", "beta3_1", "beta4_1", "beta5_1",
                   "beta1_2", "beta2_2", "beta3_2", "beta4_2", "beta5_2",
                   "beta1_3", "beta2_3", "beta3_3", "beta4_3", "beta5_3",
                   "beta1_4", "beta2_4", "beta3_4", "beta4_4", "beta5_4"),
                 c("1", "2", "3", "4", "5", "6", "7", "8"))
mus_hierarchical <- matrix(nrow = 24, ncol = length(musigmas), dimnames = dimnames)
sigmas_hierarchical <- matrix(nrow = 24, ncol = length(musigmas), dimnames = dimnames)

for (i in 1:length(musigmas)) { 
  data_hierarchical_sens <- list(N1 = nrow(male_white),
                                 N2 = nrow(male_non_white),
                                 N3 = nrow(female_white),
                                 N4 = nrow(female_non_white),
                                 x1 = subset(male_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 x2 = subset(male_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 x3 = subset(female_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 x4 = subset(female_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 y1 = male_white$GPA,
                                 y2 = male_non_white$GPA,
                                 y3 = female_white$GPA,
                                 y4 = female_non_white$GPA,
                                 musigma = musigmas[i],
                                 sigmasigma = sigmasigmas[i])
  fit_hierarchical_sens <- mod_hierarchical$sample(data_hierarchical_sens,
                                                   refresh = 0,
                                                   seed = 03091900)
  summary_sens <- fit_hierarchical_sens$summary()
  mus_hierarchical[,i] <- summary_sens$mean[2:25]
  sigmas_hierarchical[,i] <- summary_sens$sd[2:25]
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
kable(as.data.frame(mus_hierarchical), format = "latex", caption = "Sensitivity analysis of the hierarchical model: Posterior means")
kable(as.data.frame(sigmas_hierarchical), format = "latex", caption = "Sensitivity analysis of the hierarchical model: Posterior SDs")
```

The hierarchical model's sensitivity was tested similarly to the pooled model. The resulting posteriors are shown in two tables. There is slightly more variation in the posteriors of the hierarchical model when various priors are utilised, but the posteriors look quite similar nonetheless. It can be concluded that the hierarchical model, similarly to the pooled model, is not sensitive to changes in the priors.


## Posterior distributions

Next, let us inspect and visualise the posterior distributions of $\alpha$ and $\beta_k$. The table below contains the posterior distributions of the six variables for the pooled model and all 4 groups of the hierarchical model separately. All parameters are reported with the accuracy of 3 decimals.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
dimnames <- list(c("alpha", "beta1", "beta2", "beta3", "beta4", "beta5"),
                 c("pooled", "h. group 1", "h. group 2",
                   "h. group 3", "h. group 4"))
post_table <- matrix(nrow = 6, ncol = 5, dimnames = dimnames)

# Fill 1st column
for (i in 1:6) {
  mean <- summary_pooled$mean[1+i]
  sd <- summary_pooled$sd[1+i]
  post_table[i,1] <- paste0("N(", round(mean,3), ", ", round(sd,3), ")")
}
# Fill other columns
# first alphas
for (i in 1:4) {
  mean <- summary_hierarchical$mean[1+i]
  sd <- summary_hierarchical$sd[1+i]
  post_table[1, 1+i] <- paste0("N(", round(mean,3), ", ", round(sd,3), ")")
}
# then betas
for (i in 1:4) {
  for(j in 1:5) {
    mean <- summary_hierarchical$mean[5+(i-1)*5+j]
    sd <- summary_hierarchical$sd[5+(i-1)*5+j]
    post_table[j+1,i+1] <- paste0("N(", round(mean,3), ", ", round(sd,3), ")")
  }
}

kable(as.data.frame(post_table), format = "latex", caption = "Posterior distributions of the parameters for both models")
```

Let us also plot the posterior distributions.

### Pooled model

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4.5, fig.cap="Posterior distributions of the parameters of the pooled model. Note the varying x-axes."}
beta_xlims = c(1, 0.01, 0.01, 0.1, 0.1)
par(mfrow = c(3,2))

x <- seq(-2, 2, length = 1000)
y <- dnorm(x, mean = summary_pooled$mean[2], sd = summary_pooled$sd[2])
plot(x, y, type = "l", xlab = "alpha")
for (j in 1:5) {
  x <- seq(-beta_xlims[j], beta_xlims[j], length = 1000)
  y <- dnorm(x, mean = summary_pooled$mean[2+j], sd = summary_pooled$sd[2+j])
  plot(x, y, type = "l", xlab = paste0("beta", j))
}
```

The plot shows that the slope parameter $\beta_1$, multiplying the variable for high school GPA, likely differs from 0 and it the largest of the coefficients $\beta_k$. This implies that high school GPA has the most effect on college GPA, our target variable. The other $\beta$s have very narrow distributions close to 0, implying their effect on college GPA is either very small or non-existent.

### Hierarchical model

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4.5, fig.cap="Posterior distributions of the parameters of the hierarchical model. Each group's parameters are plotted in a different colour (black = group 1, blue = group 2, pink = group 3, violet = group 4). Note the varying x-axes."}
# All parameters of the hierarchical model groups are plotted in the same plots
cols = c("black", "blue", "deeppink", "darkviolet")
beta_ylims = c(4.5, 750, 700, 80, 55)
par(mfrow = c(3,2))

# Plot alphas
x <- seq(-2, 2, length = 1000)
y <- dnorm(x, mean = summary_hierarchical$mean[2],
           sd = summary_hierarchical$sd[2])
plot(x, y, type = "l", xlab = "alpha", col = cols[1], ylim = c(0, 1))
for (i in 2:4) {
  y <- dnorm(x, mean = summary_hierarchical$mean[1+i],
             sd = summary_hierarchical$sd[1+i])
  lines(x, y, type = "l", xlab = "alpha", col = cols[i])
}
# Plot betas
for (j in 1:5) {
  x <- seq(-beta_xlims[j], beta_xlims[j], length = 1000)
  y <- dnorm(x, mean = summary_hierarchical$mean[5+j], 
             sd = summary_hierarchical$sd[5+j])
  plot(x, y, type = "l", xlab = paste0("beta", j), col = cols[1], ylim = c(0, beta_ylims[j]))
  for (i in 2:4) {
    y <- dnorm(x, mean = summary_hierarchical$mean[5+(i-1)*5+j], 
               sd = summary_hierarchical$sd[5+(i-1)*5+j])
    lines(x, y, type = "l", xlab = paste0("beta", j), col = cols[i])
  }
}
```

The posteriors of the hierarchical model are quite similar to the pooled model. The distributions of the intercept terms $\alpha$ of groups 1 and 3, and of groups 2 and 4, are very similar to each other, implying that the ethnicity of the student governs the intercept term $\alpha$ in the hierarchical model. Furthermore, there is slight variation in the slope terms $\beta_k$, especially $\beta_4$ and $\beta_5$: for groups with less data, the distributions are narrower, and for groups with more data, wider.


## Model comparison

The log likelihoods of the data have been calculated in the "generated quantities" blocks of the models. They are given to the "loo" function of the "loo" package, and the individual results and model comparison are printed.

```{r, message=FALSE, warning=FALSE}
log_lik_pooled <- fit_pooled$draws("log_lik", format = "matrix")
log_lik_hierarchical <- fit_hierarchical$draws("log_lik", format = "matrix")

loo_pooled <- loo(log_lik_pooled)
loo_hierarchical <- loo(log_lik_hierarchical)

loo_pooled
loo_hierarchical
loo_compare(list("pooled" = loo_pooled, "hierarchical" = loo_hierarchical))
```

The results show that the PSIS-LOO estimate for the pooled model can be considered reliable, as all values $\hat{k} \lesssim 0.7$. The same cannot be said of the hierarchical model, and the PSIS-LOO estimate for it should be considered, or at least suspected, biased and overly optimistic.

Comparing the two models reveals that the pooled model seems to fit to the data  better than the hierarchical model (likely due to the lack of training data for the latter one). Therefore for any future predictions, the pooled model should be utilised.

\newpage

# Discussion about problems and improvements

The largest problem in the project was the amount of data. Although there was enough data for the pooled model, for the hierarchical model some of the data groups were quite small. For example, the non-white groups only contained 18 and 28 data points, and the rest of the 173 data points were distributed between white groups. 

There was large variation in the magnitudes of the variables: for example, the SATV and SATM variables had 100 times larger values than the HSGPA. Because of the magnitude difference, interpreting the coefficients $\beta_k$ was not very simple. This problem could have been solved by using variable normalisation before running the models.

There were some divergences in the hierarchical model. If there had been more time, the divergences could have been solved by analysing the the model and how it was run further. For example, the default values (number of chains, number of iterations etc.) were utilised during model fitting, and different values could be tested. Furthermore, different priors could have been tested to see if it would solve the divergence issue.


# Conclusion

Both models predicted that the high school GPA affected our target variable, college GPA, the most, as its coefficient $\beta_1$ was the largest. Other variables had either a smaller or non-existent effect. It is difficult to say for sure, as the posteriors of some of the $\beta_k$ overlap with 0, although they are more on the positive side. Furthermore, the large magnitude differences between variables make interpreting the coefficients difficult. The hierarchical model found that the ethnicity of a student governs the $\alpha$ and $\beta_2$ parameters and that gender seemed to have no effect on them. However, the parameters were quite similar across all four groups in the hierarchical model, except for a few cases when there was a large difference between the amount of data in the groups. This can be seen e.g. from $\beta_5$: the second group had the least data, and therefore the posterior of $\beta_5$ for the second group is the widest.

Based on the model comparison with LOO-CV, the pooled model describes the data better than the hierarchical model. However, both models performed quite well and the performance difference was not large. As there were various issues with the hierarchical model, e.g. divergences and $\hat{k}$ values, the pooled model should be chosen as the better model and utilised for any future predictions.


# Self-reflection

During the project, we learned to make more complicated Stan models. Additionally, we noticed that finding suitable data for data analysis is surprisingly difficult and time-consuming. Moreover, understanding the data, pre-processing it, and deciding the suitable groups for the hierarchical model took a considerable amount of time. We also noticed the importance of the amount of data: the more data we can feed the model, the better the model fits and predicts. However, we also learned that too much data can become a practical issue, as running the models with all 219 data points took quite a long time.


