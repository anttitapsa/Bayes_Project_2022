---
title: "BDA project report"
author: "Amanda Aarnio, Anni Niskanen, Antti Huttunen"
date: "`r Sys.Date()`"
output: pdf_document
---

# Introduction

```{r, warning=FALSE, message=FALSE}
library(cmdstanr)
library(bayesplot)
library(Stat2Data)
data("FirstYearGPA")
```

\newpage

# Data and Problem 

```{r, warning=FALSE, message=FALSE}
male_white <- FirstYearGPA[FirstYearGPA$Male==1 & FirstYearGPA$White==1,]
male_non_white <- FirstYearGPA[FirstYearGPA$Male==1 & FirstYearGPA$White==0,]
female_white <- FirstYearGPA[FirstYearGPA$Male==0 & FirstYearGPA$White==1,]
female_non_white <- FirstYearGPA[FirstYearGPA$Male==0 & FirstYearGPA$White==0,]
```

```{r, warning=FALSE, message=FALSE}
data_hierarchical <- list(N1 = nrow(male_white),
                            N2 = nrow(male_non_white),
                            N3 = nrow(female_white),
                            N4 = nrow(female_non_white),
                            x1 = subset(male_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            x2 = subset(male_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            x3 = subset(female_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            x4 = subset(female_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            y1 = male_white$GPA,
                            y2 = male_non_white$GPA,
                            y3 = female_white$GPA,
                            y4 = female_non_white$GPA,
                            musigma = 100,
                            sigmasigma = 10)
data_pooled <- list(N = nrow(FirstYearGPA),
                    x = subset(FirstYearGPA, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                    y = FirstYearGPA$GPA)
```

\newpage

# Models

## Pooled model

**Mathematical notation**

$$
\begin{aligned}
GPA_i &\sim N(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot HSGPA_i + \beta_2 \cdot SATV_i + \beta_3 \cdot SATM_i + \beta_4 \cdot HU_i + \beta_5 \cdot SS_i \\
\sigma &\sim N(0, 10) \\
\alpha &\sim N(0, 100) \\
\beta_k &\sim N(0, 100) \\
\end{aligned}
$$
```{r}
writeLines(readLines("pooled.stan"))
mod_pooled <- cmdstan_model("pooled.stan")
fit_pooled <- mod_pooled$sample(data_pooled, refresh = 2000, seed = 03091900)
```

### Hierarchical model

**Mathematical notation**

$$
\begin{aligned}
GPA_{ij} &\sim N(\mu_{ij}, \sigma) \\
\mu_{ij} &= \alpha_j + \beta_{1j} \cdot HSGPA_i + \beta_{2j} \cdot SATV_i + \beta_{3j} \cdot SATM_i + \beta_{4j} \cdot HU_i + \beta_{5j} \cdot SS_i \\
\sigma &\sim N(0, 10) \\
\alpha_j &\sim N(\mu_{\alpha}, \sigma_{\alpha}) \\
\beta_{1j} &\sim N(\mu_{\beta_1}, \sigma_{\beta_1}) \\
\beta_{2j} &\sim N(\mu_{\beta_2}, \sigma_{\beta_2}) \\
\beta_{3j} &\sim N(\mu_{\beta_3}, \sigma_{\beta_3}) \\
\beta_{4j} &\sim N(\mu_{\beta_4}, \sigma_{\beta_4}) \\
\beta_{5j} &\sim N(\mu_{\beta_5}, \sigma_{\beta_5}) \\
\mu_{\alpha} &\sim N(0,100) \\
\sigma_{\alpha} &\sim N(0,10) \\
\mu_{\beta_k} &\sim N(0,100) \\
\sigma_{\beta_k} &\sim N(0,10) \\
\end{aligned}
$$

```{r}
writeLines(readLines("hierarchical.stan"))
mod_hierarchical <- cmdstan_model("hierarchical.stan")
fit_hierarchical <-mod_hierarchical$sample(data_hierarchical, refresh = 2000,
                                           seed = 03091900)
```

Weakly informative normal priors were used in both models. N(0,10) was the prior distribution for $\sigma$ in both models. In the pooled model, N(0,100) was the prior for both $\alpha$ and all $\beta_k$. The hierarchical model had similar priors: N(0,100) for all means $\mu_{\alpha}$ and $\mu_{\beta_k}$, and N(0,10) for all standard deviations $\sigma_{\alpha}$ and $\sigma_{\beta_k}$. We thought these priors reasonable for multiple reasons. Firstly, they all center on 0, which was thought wisest as we have no information whether the intercept $\alpha$ or the slopes $\beta_k$ should be positive or negative. Secondly, the standard deviations, 100 for the means and 10 for the standard deviations, were considered large enough to produce wide enough (but not too wide) prior distributions.

As can be seen from the above code lines, default values were used for running the MCMC chains. That is, 4 chains of 2000 iterations were run, and the first 1000 iterations of each chain were considered warm-up.

\newpage

# Analysis and Results

## Converge diagnostics

### Pooled model

```{r}
sum(fit_pooled$summary()$rhat > 1.01)
sum(fit_pooled$summary()$rhat < 0.99)
```

No divergence

### Hierarchical model

```{r}
sum(fit_hierarchical$summary()$rhat > 1.01)
sum(fit_hierarchical$summary()$rhat < 0.99)
```

Divergence: 218 of 4000 (5%)

ESS:
To-do 
Check what's good ess!?

Questions for TA:
- What does it mean that values are higher that 1.01?
- What's the difference between ess_bulk and ess_tail?
- What's ESS? We had difficulties to understand course book.

## Posterior predictive checks

### Pooled model

```{r}
y <- FirstYearGPA$GPA
ypred <- fit_pooled$draws("ypred", format = "matrix")

ppc_hist(y, ypred[1:48,])
ppc_dens_overlay(y, ypred[1:100,])
ppc_ecdf_overlay(y, ypred[1:100,])
```

### Hierarchical model

```{r}
y1 <- data_hierarchical$y1
ypred1 <- fit_hierarchical$draws("ypred1", format = "matrix")

ppc_hist(y1, ypred1[1:48,])
ppc_dens_overlay(y1, ypred1[1:100,])
ppc_ecdf_overlay(y1, ypred1[1:100,])
```

```{r}
y2 <- data_hierarchical$y2
ypred2 <- fit_hierarchical$draws("ypred2", format = "matrix")

ppc_hist(y2, ypred2[1:48,])
ppc_dens_overlay(y2, ypred2[1:100,])
ppc_ecdf_overlay(y2, ypred2[1:100,])
```

```{r}
y3 <- data_hierarchical$y3
ypred3 <- fit_hierarchical$draws("ypred3", format = "matrix")

ppc_hist(y3, ypred3[1:48,])
ppc_dens_overlay(y3, ypred3[1:100,])
ppc_ecdf_overlay(y3, ypred3[1:100,])
```

```{r}
y4 <- data_hierarchical$y4
ypred4 <- fit_hierarchical$draws("ypred4", format = "matrix")

ppc_hist(y4, ypred4[1:48,])
ppc_dens_overlay(y4, ypred4[1:100,])
ppc_ecdf_overlay(y4, ypred4[1:100,])
```

## Prior sensitivity analysis

Original
Normal(0, 100) + Normal(0, 10)

Tests
Normal(0, 1000) + Normal(0,10)
Normal(0, 50) + Normal(0,10)
Normal(0, 10) + Normal(0,10)
Normal(0, 1) + Normal(0, 10)
  and vice versa
Normal (0,100) + Normal (0,100)
Normal (0,100) + Normal (0,50)
Normal (0,100) + Normal (0,1)

\newpage

# Discussion

\newpage

# Conclusion

\newpage

# Self-reflection




