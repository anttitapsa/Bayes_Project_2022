---
title: "BDA project report"
author: "Amanda Aarnio, Anni Niskanen, Antti Huttunen"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    toc: yes
    toc_depth: 1
---

\newpage

# Introduction

Can the grade point averages (GPAs) of first-year college students be predicted based on high school GPA and other factors. This report presents a solution for this problem by applying two statiscal models to predict the first-year college GPA. The models are constracted and compared according to Bayesian data analysis concepts.

```{r libraries, warning=FALSE, message=FALSE}
library(cmdstanr)
library(bayesplot)
library(gridExtra)
library(loo)
library(Stat2Data)

data("FirstYearGPA")
```

\newpage

# Data and Problem 

Description of the data and the analysis problem. Provide information where the data was obtained, 

The data contains information from a sample of 219 first year students at a midwestern college in 1996. The data has 10 variables:

```
GPA           First-year college GPA on a 0.0 to 4.0 scale
HSGPA         High school GPA on a 0.0 to 4.0 scale
SATV          Verbal/critical reading SAT score
SATM          Math SAT score
Male          1= male, 0= female
HU            Number of credit hours earned in humanities courses in high school
SS            Number of credit hours earned in social science courses in high school
FirstGen      1= student is the first in her or his family to attend college, 0=otherwise
White         1= white students, 0= others
CollegeBound  1=attended a high school where >=50% students intended to go on to college, 0=otherwise
```
The data can be obtained from \url{https://vincentarelbundock.github.io/Rdatasets/doc/Stat2Data/FirstYearGPA.html}. Our models use all numerical values (HSGPA, SATV, SATM, HU and SS) to predict the first-year college GPA (GPA). In addition, the hierarchical model uses categorical variables Male and White to group the data into four different groups and predicts the GPA with group-level predictors.

```{r data_barplot}
counts <- table(White=FirstYearGPA$White, Male=FirstYearGPA$Male)
barplot(counts,
  xlab="Gender", col=c("darkblue","red"),
  legend = c('Non-White', 'White'), ylab = 'Count', names=c('Female','Male'))
```

\newpage

# Models

In this project, two different models are utilised: pooled and hierarchical model. Both are are covered in following sections. The mathematical notations, Stan implementations, and stan model runs are included in the sections. The both of the models follow the linear Gaussian model whose expected values are constructed using linear function with variables found in data (HSGPA, SATV, SATM, HU, and SS) and parameters $\alpha$, $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$.   

## Pooled model

In pooled model, all the expected values are constructed using the common parameters $\alpha$ and $\beta$s with weakly informative priors. All the GPAs have common $\sigma$. 

**Mathematical notation**

$$
\begin{aligned}
GPA_i &\sim N(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \cdot HSGPA_i + \beta_2 \cdot SATV_i + \beta_3 \cdot SATM_i + \beta_4 \cdot HU_i + \beta_5 \cdot SS_i \\
\sigma &\sim N(0, 10) \\
\alpha &\sim N(0, 100) \\
\beta_k &\sim N(0, 100) \\
\end{aligned}
$$

**Stan code**

```{r}
writeLines(readLines("pooled.stan"))
```

**Running the model**


```{r data_pooled, warning=FALSE, message=FALSE}
data_pooled <- list(N = nrow(FirstYearGPA),
                    x = subset(FirstYearGPA, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                    y = FirstYearGPA$GPA,
                    musigma = 100,
                    sigmasigma = 10)
```

```{r pooled_model_fit}
mod_pooled <- cmdstan_model("pooled.stan")
fit_pooled <- mod_pooled$sample(data_pooled, refresh = 0, seed = 03091900)
```

## Hierarchical model

In Hierarchical model, students have been divided in four different groups: white males, white females, non white males, and non white females. All students in all the groups have own parameters $\alpha$s and $\beta$s which are constructed using common hyperparameters for all the different groups.

**Mathematical notation**

$$
\begin{aligned}
GPA_{ij} &\sim N(\mu_{ij}, \sigma) \\
\mu_{ij} &= \alpha_j + \beta_{1j} \cdot HSGPA_i + \beta_{2j} \cdot SATV_i + \beta_{3j} \cdot SATM_i + \beta_{4j} \cdot HU_i + \beta_{5j} \cdot SS_i \\
\sigma &\sim N(0, 10) \\
\alpha_j &\sim N(\mu_{\alpha}, \sigma_{\alpha}) \\
\beta_{1j} &\sim N(\mu_{\beta_1}, \sigma_{\beta_1}) \\
\beta_{2j} &\sim N(\mu_{\beta_2}, \sigma_{\beta_2}) \\
\beta_{3j} &\sim N(\mu_{\beta_3}, \sigma_{\beta_3}) \\
\beta_{4j} &\sim N(\mu_{\beta_4}, \sigma_{\beta_4}) \\
\beta_{5j} &\sim N(\mu_{\beta_5}, \sigma_{\beta_5}) \\
\mu_{\alpha} &\sim N(0,100) \\
\sigma_{\alpha} &\sim N(0,10) \\
\mu_{\beta_k} &\sim N(0,100) \\
\sigma_{\beta_k} &\sim N(0,10) \\
\end{aligned}
$$

**Stan code**

```{r}
writeLines(readLines("hierarchical.stan"))
```

**Running the model


```{r data_hierarchical, warning=FALSE, message=FALSE}
male_white <- FirstYearGPA[FirstYearGPA$Male==1 & FirstYearGPA$White==1,]
male_non_white <- FirstYearGPA[FirstYearGPA$Male==1 & FirstYearGPA$White==0,]
female_white <- FirstYearGPA[FirstYearGPA$Male==0 & FirstYearGPA$White==1,]
female_non_white <- FirstYearGPA[FirstYearGPA$Male==0 & FirstYearGPA$White==0,]

data_hierarchical <- list(N1 = nrow(male_white),
                            N2 = nrow(male_non_white),
                            N3 = nrow(female_white),
                            N4 = nrow(female_non_white),
                            x1 = subset(male_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            x2 = subset(male_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            x3 = subset(female_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            x4 = subset(female_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                            y1 = male_white$GPA,
                            y2 = male_non_white$GPA,
                            y3 = female_white$GPA,
                            y4 = female_non_white$GPA,
                            musigma = 100,
                            sigmasigma = 10)
```

```{r hierarchical_model_fit}
mod_hierarchical <- cmdstan_model("hierarchical.stan")
fit_hierarchical <-mod_hierarchical$sample(data_hierarchical, refresh = 0,
                                           seed = 03091900)
```

Weakly informative normal priors were used in both models. N(0,10) was the prior distribution for $\sigma$ in both models. In the pooled model, N(0,100) was the prior for both $\alpha$ and all $\beta_k$. The hierarchical model had similar priors: N(0,100) for all means $\mu_{\alpha}$ and $\mu_{\beta_k}$, and N(0,10) for all standard deviations $\sigma_{\alpha}$ and $\sigma_{\beta_k}$. We thought these priors reasonable for multiple reasons. Firstly, they all center on 0, which was thought wisest as we have no information whether the intercept $\alpha$ or the slopes $\beta_k$ should be positive or negative. Secondly, the standard deviations, 100 for the means and 10 for the standard deviations, were considered large enough to produce wide enough (but not too wide) prior distributions.

As can be seen from the above code lines, default values were used for running the MCMC chains. That is, 4 chains of 2000 iterations were run, and the first 1000 iterations of each chain were considered warm-up.

\newpage

# Analysis and Results

## Converge diagnostics

### Pooled model

```{r pooled_rhat}
summary_pooled <- fit_pooled$summary()
sum(summary_pooled$rhat > 1.01)
sum(summary_pooled$rhat < 0.99)
```

```{r pooled_ess}
mean(summary_pooled$ess_tail)
sum(summary_pooled$ess_tail<400)
mean(summary_pooled$ess_bulk)
sum(summary_pooled$ess_bulk<400)
```

None of the $\hat{R}$-values for the pooled model exceeds 1.01 which indicates that there are no problems with the convergence of the set of simulated chains. The effective sample size (ESS) is over 400 (recommended threshold with four parallel chains, 100 per chain) for all of the ess_bulk and ess_tail values which indicates that the $\hat{R}$-estimate can be trusted to make decisions about convergence and the quality of the chains. In addition, the model does not observe divergence.

### Hierarchical model

```{r hierarchical_rhat}
summary_hierarchical <- fit_hierarchical$summary()
sum(summary_hierarchical$rhat > 1.01)
sum(summary_hierarchical$rhat < 0.99)
```

```{r hierarchical_ess}
mean(summary_hierarchical$ess_tail)
sum(summary_hierarchical$ess_tail<400)
mean(summary_hierarchical$ess_bulk)
sum(summary_hierarchical$ess_bulk<400)
```

For the hierarchical model a few $\hat{R}$-values exceed 1.01 and a few ESS-values are below 400. The model gives divergence of approximately 5%. These facts can indicate that there might be some problems with the convergence. On the other hand, these values do not differ significantly from the desired values the probability of having problems with convergence is small.


## Posterior predictive checks

48 histograms of replications of the data (created based on the posterior predictive distributions) are shown for all the models. In addition, there is over layed distributions and empirical cumulative distribution (ecdf) functions of 100 replications in the two with the original distribution and ecdf. These different plots of replication are compared to the plots of the original data to investigate, how good predictions fitted model can make. The closer the replications are original data, the better the predictions are.

### Pooled model

```{r pooled_posterior, fig.height=7.2}
y <- FirstYearGPA$GPA
ypred <- fit_pooled$draws("ypred", format = "matrix")

grid.arrange( ppc_hist(y, ypred[1:48,]),
              ppc_dens_overlay(y, ypred[1:100,]), 
              ppc_ecdf_overlay(y, ypred[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

### Hierarchical model

```{r hierarchical_posterior1, fig.height=7.2}
y1 <- data_hierarchical$y1
ypred1 <- fit_hierarchical$draws("ypred1", format = "matrix")

grid.arrange(ppc_hist(y1, ypred1[1:48,]),
ppc_dens_overlay(y1, ypred1[1:100,]),
ppc_ecdf_overlay(y1, ypred1[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

```{r hierarchical_posterior2, fig.height=7.2}
y2 <- data_hierarchical$y2
ypred2 <- fit_hierarchical$draws("ypred2", format = "matrix")

grid.arrange(ppc_hist(y2, ypred2[1:48,]),
ppc_dens_overlay(y2, ypred2[1:100,]),
ppc_ecdf_overlay(y2, ypred2[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

```{r hierarchical_posterior3, fig.height=7.2}
y3 <- data_hierarchical$y3
ypred3 <- fit_hierarchical$draws("ypred3", format = "matrix")

grid.arrange(ppc_hist(y3, ypred3[1:48,]),
ppc_dens_overlay(y3, ypred3[1:100,]),
ppc_ecdf_overlay(y3, ypred3[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

```{r hierarchical_posterior4, fig.height=7.2}
y4 <- data_hierarchical$y4
ypred4 <- fit_hierarchical$draws("ypred4", format = "matrix")

grid.arrange(ppc_hist(y4, ypred4[1:48,]),
ppc_dens_overlay(y4, ypred4[1:100,]),
ppc_ecdf_overlay(y4, ypred4[1:100,]),
              layout_matrix = matrix(c(1,2,1,3), nrow = 2), nrow=2)
```

## Prior sensitivity analysis

Original priors:
Normal(0, 100) + Normal(0, 10)

Priors to test:
Normal(0, 1000) + Normal(0,10)
Normal(0, 50) + Normal(0,10)
Normal(0, 10) + Normal(0,10)
Normal(0, 1) + Normal(0,10)
  and vice versa
Normal (0,100) + Normal (0,100)
Normal (0,100) + Normal (0,50)
Normal (0,100) + Normal (0,1)

### Pooled model

```{r}
musigmas <- c(100, 1000, 50, 10, 1, 100, 100, 100)
sigmasigmas <- c(10, 10, 10, 10, 10, 100, 50, 1)
mus_pooled <- matrix(nrow = 6, ncol = length(musigmas))
sigmas_pooled <- matrix(nrow = 6, ncol = length(musigmas))

for (i in 1:length(musigmas)) {
  data_pooled_sens <- list(N = nrow(FirstYearGPA),
                         x = subset(FirstYearGPA, select = c('HSGPA', 'SATV',
                                                             'SATM','HU','SS')),
                         y = FirstYearGPA$GPA,
                         musigma = musigmas[i],
                         sigmasigma = sigmasigmas[i])
  fit_pooled_sens <- mod_pooled$sample(data_pooled_sens, refresh = 0,
                                       seed = 03091900)
  summary_sens <- fit_pooled_sens$summary()
  mus_pooled[,i] <- summary_sens$mean[2:7]
  sigmas_pooled[,i] <- summary_sens$sd[2:7]
}
```

```{r}
dimnames <- list(c("alpha", "beta1", "beta2", "beta3", "beta4", "beta5"),
                 c("1", "2", "3", "4", "5", "6", "7", "8"))
dimnames(mus_pooled) <- dimnames
dimnames(sigmas_pooled) <- dimnames

as.data.frame(mus_pooled)
as.data.frame(sigmas_pooled)
```

Regardless of the used priors, all posteriors for $\alpha$ and $\beta_k$ are the same. Therefore our pooled model is not sensitive to changes in the priors, as long as wide enough ones are utilised.

Posteriors plotted:

```{r}
par(mfrow = c(3,2))
x <- seq(-2, 2, length = 1000)
y <- dnorm(x, mean = summary_pooled$mean[2], sd = summary_pooled$sd[2])
plot(x, y, type = "l", xlab = "alpha")
for (i in 1:5) {
  x <- seq(-1, 1, length = 1000)
  y <- dnorm(x, mean = summary_pooled$mean[2+i], sd = summary_pooled$sd[2+i])
  plot(x, y, type = "l", xlab = paste0("beta", i))
}
```

### Hierarchical model

```{r}
musigmas <- c(100, 1000, 50, 10, 1, 100, 100, 100)
sigmasigmas <- c(10, 10, 10, 10, 10, 100, 50, 1)
mus_hierarchical <- matrix(nrow = 24, ncol = length(musigmas))
sigmas_hierarchical <- matrix(nrow = 24, ncol = length(musigmas))

for (i in 1:length(musigmas)) {
  data_hierarchical_sens <- list(N1 = nrow(male_white),
                                 N2 = nrow(male_non_white),
                                 N3 = nrow(female_white),
                                 N4 = nrow(female_non_white),
                                 x1 = subset(male_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 x2 = subset(male_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 x3 = subset(female_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 x4 = subset(female_non_white, select = c('HSGPA', 'SATV', 'SATM','HU','SS')),
                                 y1 = male_white$GPA,
                                 y2 = male_non_white$GPA,
                                 y3 = female_white$GPA,
                                 y4 = female_non_white$GPA,
                                 musigma = musigmas[i],
                                 sigmasigma = sigmasigmas[i])
  fit_hierarchical_sens <- mod_hierarchical$sample(data_hierarchical_sens,
                                                   refresh = 0,
                                                   seed = 03091900)
  summary_sens <- fit_hierarchical_sens$summary()
  mus_hierarchical[,i] <- summary_sens$mean[2:25]
  sigmas_hierarchical[,i] <- summary_sens$sd[2:25]
}
```

```{r}
dimnames <- list(c("alpha1", "alpha2", "alpha3", "alpha4",
                   "beta1_1", "beta2_1", "beta3_1", "beta4_1", "beta5_1",
                   "beta1_2", "beta2_2", "beta3_2", "beta4_2", "beta5_2",
                   "beta1_3", "beta2_3", "beta3_3", "beta4_3", "beta5_3",
                   "beta1_4", "beta2_4", "beta3_4", "beta4_4", "beta5_4"),
                 c("1", "2", "3", "4", "5", "6", "7", "8"))
dimnames(mus_hierarchical) <- dimnames
dimnames(sigmas_hierarchical) <- dimnames

as.data.frame(mus_hierarchical)
as.data.frame(sigmas_hierarchical)
```

```{r}
# Each group's parameters get its own plots (4 3x2 plots in total)
for (i in 1:4) {
  par(mfrow = c(3,2))
  x <- seq(-2, 2, length = 1000)
  y <- dnorm(x, mean = summary_hierarchical$mean[1+i], sd = summary_hierarchical$sd[1+i])
  plot(x, y, type = "l", xlab = "alpha")
  for (j in 1:5) {
    x <- seq(-1, 1, length = 1000)
    y <- dnorm(x, mean = summary_hierarchical$mean[5+(i-1)*5+j], sd = summary_hierarchical$sd[5+(i-1)*5+j])
    plot(x, y, type = "l", xlab = paste0("beta", i))
  }
}
```

## Model comparison

```{r}
dimnames <- list(c("alpha", "beta1", "beta2", "beta3", "beta4", "beta5"),
                 c("pooled", "h. group 1", "h. group 2",
                   "h. group 3", "h. group 4"))
post_table <- matrix(nrow = 6, ncol = 5, dimnames = dimnames)

# Fill 1st column
for (i in 1:6) {
  mean <- summary_pooled$mean[1+i]
  sd <- summary_pooled$sd[1+i]
  post_table[i,1] <- paste0("N(", round(mean,3), ", ", round(sd,3), ")")
}
# Fill other columns
# first alphas
for (i in 1:4) {
  mean <- summary_hierarchical$mean[1+i]
  sd <- summary_hierarchical$sd[1+i]
  post_table[1, 1+i] <- paste0("N(", round(mean,3), ", ", round(sd,3), ")")
}
# then betas
for (i in 1:4) {
  for(j in 1:5) {
    mean <- summary_hierarchical$mean[5+(i-1)*5+j]
    sd <- summary_hierarchical$sd[5+(i-1)*5+j]
    post_table[j+1,i+1] <- paste0("N(", round(mean,3), ", ", round(sd,3), ")")
  }
}

as.data.frame(post_table)
```

```{r}
log_lik_pooled <- fit_pooled$draws("log_lik", format = "matrix")
log_lik_hierarchical <- fit_hierarchical$draws("log_lik", format = "matrix")

loo_pooled <- loo(log_lik_pooled)
loo_hierarchical <- loo(log_lik_hierarchical)

loo_pooled
loo_hierarchical
loo_compare(list("pooled" = loo_pooled, "hierarchical" = loo_hierarchical))
```

\newpage

# Discussion about problems and improvements

The overall problem in the project was that there was not enough data. For example in hierarchical model, some of the data groups were quite small: th group of non white groups only contained 18 and 28 data, and the rest of the 173 data points were distributed between white groups. In addition, there was big variation in the magnitudes of the variables (HSGPA etc.). This could have been solved by using normalisation.

There were divergences in the hierarchical model. If there was more time, the divergences could have been solved by analysing more the running of the model; The default values (the number of the chains, the number of the iterations etc.) were utilised during the model fitting. Furthermore, testing out different priors to solve the divergence issue.

\newpage

# Conclusion

Based on the analysis, the pooled model describes the data better than the hierarchical model. However, both models performed well. The hierarchical model describes the white and non white students differently. So according to the hierarchical model, there were more differences between white and non-white students than between male and female students. This can be seen most clearly from the posterior distributions of the $\alpha$s.   

\newpage

# Self-reflection

During the project, we noticed that finding the data is difficult and time consuming. Moreover, the pre-processing of the data and deciding the suitable data groups for the hierarchical model was surprisingly time consuming. We also noticed the importance of the amount of the data: more data, better model.

We learned to make more complicated stan models.  


